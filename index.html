<!DOCTYPE html>
<html>
    <head>
        <title>Psychonomics'25</title>
        <link href='./psychonomics_25.css' rel='stylesheet'>
    </head>
    <body>
        <h1>Psychonomics 2025 highlights</h1>
        <div class="intro"></div>
        <p>
         This page gathers notes from a few Psychonomics 2025 talks that I found interesting. 
         Click any link below to jump directly to that talk.
        </p>
        <div class="contents">
            <ul>
                <li>üßê <a href="#talk1">How do we judge what others will learn?</a></li>
                <li>üë§ <a href="#talk2">Can knowing who someone is tell you what they know?</a></li>
                <li>üß† <a href="#talk3">How do we infer others' confidence and competence?</a></li>
                <li>üîç <a href="#talk4">What's the best way to ask metacognitive monitoring questions?</a></li>
                <li>üîÆ <a href="#talk5">What makes conspiracies believable?</a></li>
                <li>üé≤ <a href="#talk6">Which psychological mechanisms shape our everyday decisions?</a></li>

            </ul>
        </div>
        <div class="transition">
            <p class="transition">A few of this year's metacognition talks had a similar theme - 
                social metacognition - our ability to think about others' learning, knowledge, 
                and confidence. To succeed in a social world, we must be able to accurately 
                estimate what others know.  Estimating others' knowledge is particularly 
                important for educators so that they can predict topic difficulty, 
                provide support, and adapt lessons effectively. 
                <strong>So, how do we judge what others will learn?</strong>
            </p>
        </div>
        <div class="talk" id="talk1">
        <h3>Making Judgments of Learning (JOLs) 
            for Oneself Versus Others: A Review 
            and Proposed Model</h3>
            <p class="author"><em>Barry Wei, Nicholas C. Soderstrom, & Michelle L. Meade, 
            Montana State University</em></p>
            <p> Similar to <span class="citation" data-citation="Koriat, 1997">JOLs for self</span>, 
                    JOL for others rely on various cues</p>
            <ul> 
                <li> We can predict others' memory 
                    based on intrinsic cues (e.g., font size) and extrinsic cues 
                    (e.g., retention interval)
                </li>
                <li>Judgement accuracy is higher when we have access to
                         the learner's subjective experiences such as 
                         their past performance</li>
                <li> We might interpret cues differently 
                            for self vs. others</li>
                     <ul>
                        <li>e.g. for self longer study time on an item is often interpreted 
                                    as difficulty while for others, 
                                    we interpret it as more time = more effort = better memory.</li>
                    </ul>
                <li>Judgements can be influenced by other social cues such as 
                    student engagement, likability, demographic factors</li>
                <li>JOLs for others are also subject to misleading cues:</li>
                    <ul>
                        <li>Sometimes individuals judge others' knowledge in accordance 
                            with their own knowledge, which is termed 
                            the <em>curse-of-knowledge effect</em></li>
                    </ul>
            </ul>
            <a href="https://link.springer.com/article/10.3758/s13423-025-02816-0" 
            target="_blank"><strong> üîó See for more</strong></a>
        </div>
        <div class="transition">
            <p class="transition">So, we use various cues, including social cues to judge what others know. 
                Beyond educational settings, this might also have implications in organizational 
                settings such as managing a project and/or delegating tasks to others.
                 Perhaps a more relatable example is trying to ‚Äúgauge the room‚Äù when putting 
                 together a presentation so that we are not under- or over-explaining. 
                 One of such social cues that affect our judgements is demographics. 
                 <strong>So, can having background information about others help?</strong>
            </p>
        </div>
          <div class="talk" id="talk2">
            <h3>Demographic Information Improves Predictions of Others' Knowledge</h3>
            <p class="author"><em>Jonathan Tullis, University of Arizona</em></p>
            <p></p>
            <ul>
                <li><strong>Anchoring-and-adjustment:</strong> We might not be able to ignore
                     what we already know when predicting a others' knowledge</li>
                <li><strong>Lack of cues about others:</strong> We might simply 
                lack knowledge or cues about others so we default 
                to using our own knowledge as a reference point.</li>
            </ul>
            <p><strong>üî¨ Study 1</strong>: If inaccuracies in judgments stem from over-reliance on personal knowledge, 
                reducing that reliance should improve the resolution of judgments. Researchers 
                compared two conditions:</p>
            <ul>
                <li><strong>Answer-before:</strong> Estimate (on a scale of 0% to 100%) others'
                    knowledge <em>before</em> answering trivia yourself.</li>
                <li><strong>Answer-after:</strong> Estimate others' knowledge <em>after</em> answering.</li>
            </ul>
        <p><strong>‚úÖ Results:</strong> Participants in the answer-after condition used 
            their own experiences more heavily than in the answer-before condition.
        </p>
        <p><strong>üî¨ Study 2:</strong> If we simply lack cues about others' knowledge, 
            providing relevant information should help.
        </p>
        <p><strong>‚úÖ Results:</strong> With demographic cues, participants
            relied less on their own knowledge when estimating what others knew.
        </p>
        <p><strong>üí° Theoretical implications:</strong> Future research needs to examine how people
            integrate and prioritize multiple cueslike past performance, 
            social perceptions, and task difficulty when judging others' learning. 
            Future studies can also examine which cues are more diagnostic than others, 
            especially given that JOLs for others are influenced by both the judge's 
            experiences and commonly held beliefs about learning.
        </p>
        <a href="https://link.springer.com/article/10.3758/s13421-022-01382-3" target="_blank"><strong> 
            üîó See for more</strong></a>
        </div>
        <div class="transition">
            <p class="transition"> In addition to forming impressions of others' cognitive states, we often perceive their metacognitive states 
                (aka how confident others are in their beliefs and decisions) which has 
                implications for <span class="citation" data-citation="Pescetelli & Young, 2019">advice taking</span>, <span class="citation" data-citation="Bahreneit et al, 2010">decision making</span>, 
                and <span class="citation" data-citation="Candrian & Scherer, 2022">task delegation</span>. <strong>How do we infer others‚Äô 
                confidence and competence?</strong>
            </p>
        </div>
        <div class="talk" id="talk3">
            <h3><strong>Social Metacognition: The Role of Confidence Calibration in Competence Impressions</strong></h3>
            <p class="author"><em>Clara Colombatto, University of Waterloo, Stephen Fleming, University College London</em></p>
            <p> Do attributions of confidence depend on cues such as task difficulty, observed accuracy, 
                and observed response time?</p>
            <p><strong>üî¨ Study 1: </strong> In phase one, participants make perceptual decisions regarding which of two patches containing a Gabor stimulus. 
                    In the second phase, participants observe other agents make these decisions (accuracy and RTs varied).</p>
            <p><strong>‚úÖ Results:</strong> Participants inferred higher confidence 
                on easier trials (where the target was presented at higher contrast).</p>
            <ul>
                <li>Participants also inferred higher confidence on trials where agents
                    responded more quickly.</li>    
                <li>RTs had a larger effect for observers inferring others' confidence vs. for one's own confidence</li>
                <li> There was no relationship between metacognitive accuracy for self and others</li>
                    </ul>
            </ul>
            <p><strong>üî¨ Study 2:</strong> Participants observe agent A vs. B's performance on Gabor patches and their 
                confidence rating. Both agents have overall same accuracy but different calibration on each trial.</p> 
            <p><strong>‚úÖ Results:</strong> Participants rated calibrated agents as more competent, 
                even for agents who had lower accuracy</p>
            <p><strong>üí° Takeways:</strong> So competence might not just be how good someone
                is at something (overall performance) but more about calibrated confidence. People might be 
                willing to take a trade off between accuracy and calibration and might prefer more calibrated /less 
                knowledgeable agents.
            </p>
            <p><strong>üöÄ AI applications:</strong> <em>How do people  perceive confidence of AI agents?</em> Other findings
                 from Study 1 show that participants assigned higher confidence to the decisions of artificial 
            agents, compared to those of performance-matched humans. In all Experiments participants reported that the computer 
            algorithm was confident in each choice, and overall more trustworthy and competent compared to the matched human counterparts.</p>
         <a href="https://osf.io/preprints/psyarxiv/mjx2v_v2?view_only=3d3b5cecb629486a9ace79f9be6d558d" target="_blank"><strong>
            üîó See for more</strong></a>

        </div>
        <div class="transition">
            <p class="transition"> Research in metacognition often emphasizes importance of accurate self-assessment or monitoring
                 in self-regulated learning and professional settings, with applications to designing self-reflection 
                 exercises in academic or professional training contexts. <span class="citation" data-citation="Eva & Regehr, 2011">Studies</span> show that there is a difference between 
                 self-assessment at the global level (e.g., ‚ÄúHow good a physician am I?‚Äù) versus self-monitoring of specific 
                 topic areas (e.g., ‚ÄúHow much do I know about hypertension?‚Äù). <strong>What's the right way to ask self-assessment
                  questions?</strong>
            </p>
        </div>
        <div class="talk" id="talk4">
            <h3><strong>Metacognitive Monitoring of Hierarchically Organized Domains: 
                Specificity Benefits Resolution but Impairs Calibration</strong></h3>
            <p class="author"><em>Scott Fraundorf, University of Pittsburgh</em></p>
            <p><strong>üî¨ Study</strong></p>
            <ul>
                <li>Participants read texts on three topics (Dinosaurs, Acupuncture, Comets) 
                    and made confidence judgments at three levels: specific subtopics, topics, and globally(all material). 
                </li>
                <li>Both calibration and resolution measures were calculated at the subsection level, 
                    then at the text level by averaging the subsections within a text, 
                    then at the global levels by averaging the texts</li>
            </ul>
            <p><strong>‚úÖ Results:</strong> People are better calibrated when making global judgements, 
                however, resolution is better for subtopic questions. <strong>Why?</strong></p>
            <ul>
                <li><strong>Calibration</strong> might be influenced the overall task structure 
                    (e.g., the total number of studied items, the type of test, and the number 
                    of practice trials).</li>
                <li><strong>Resolution</strong> might be shaped comparisons between individual items 
                    (i.e., comparing the difficulty of one target to other targets in the list) .</li>
            </ul>
            <p><strong>üí° Theoretical implications:</strong> We cannot assume metacognitive accuracy 
                (e.g., calibration, resolution) generalizes across different levels of specificity.
                 Do people utilize different cues when making different levels of monitoring judgements? 
                 Do we want learners to identify specific knowledge gaps (prioritize resolution) or have
                  realistic overall confidence (prioritize calibration)? </p>
            <a href="https://link.springer.com/article/10.1186/s41235-023-00511-z" target="_blank"><strong>
                üîó See for more</strong></a>
        </div>
        <div class="transition">
            <p class="transition">You may be familiar with research that examines which individual differences make people more 
                likely to believe in conspiracy theories. However, relatively less is known about what makes 
                conspiracy theories compelling explanations. Given that interventions aimed at debunking 
                conspiracy theories often involve telling participants factual or scientific explanations 
                for a given phenomenon, it may be important to first ask <strong>what makes a believable explanation?</strong></p>
        </div>
        <div class="talk" id="talk5">
            <h3><strong>The Talk the Government Doesn't Want You to Hear: 
                The Explanatory Nature of Conspiracy Theories.</strong></h3>
            <p class="author"><em>Jessecae Marsh, Samantha Kleinberg</em></p>
            <p>Research suggests 
                that we prefer simple and probable explanations with appropriate scope</p>
            <ul>
                <li><strong>Large explanatory scope:</strong> a diagnosis that can explain all of a patient's
                    observed symptoms is preferred</li>
                <li><strong>Latent scope:</strong> an explanation that can explain what is currently observed 
                    but not everything that could possibly be observed</li>
            </ul>
            <p>Conspiracies often have secret actors controlling many events 
                    (broad scope) but they can also be oddly specific to single events (narrow scope)</p>  
            <p><strong>üî¨ Study: </strong>Compare conspiracy theories with fact-based explanations across five domains 
                (UFOs, celebrities, health and political events)</p>
            <ul>
                <li>Participants rate each explanation on:</li>
                    <ul>
                        <li><strong>Explanatory scope:</strong> well does this explanation account for the known facts about the topic?</li>
                        <li><strong>Latent scope:</strong> how well could this explanation account
                             for new information that might arise about the topic?</li>
                        <li><strong>Complexity and believability</strong></li>
                    </ul>
            </ul>
            <p><strong>‚úÖ Results:</strong></p>
            <ul> 
                <li>Conspiracy explanations were rated as having smaller explanatory and latent scope than fact-based 
                explanations (same pattern held regardless of domain)</li>
                <li>The more an account seemed to explain the topic (explanatory scope) and acommodate new information
                     (latent scope) the more believable it was.</li>
            </ul>
            <p>People want the same things from their conspiracies that they do from their facts:
                 to explain both what is already known and new information that may arise</p>
            <p><strong>üí° Theoretical implications:</strong> It is possible that perceiving an explanation as having a large scope
                 promotes believing that explanation. Alternatively, finding an explanation 
                 believable might promote seeing the scope as larger.</p>
            <a href="https://escholarship.org/content/qt7gc219mf/qt7gc219mf.pdf" target="_blank"><strong>
                üîó See for more</strong></a>
        </div>
        <div class="transition">
            <p class="transition">Most studies examining decisions under risk and uncertainty focus on how people interpret monetary outcomes 
                and probabilities, without taking into account the multidimensional nature of real-life decisions. 
                I liked this talk because the researchers asked what are all the cues affecting real-world decisions? 
                They created a comprehensive taxonomy of various <strong> psychological mechanisms (e.g. goals, deadlines, effort)
                 that affect decision-making under uncertainty </strong> based on previous research and theories. More interestingly, 
                 they use LLMs to ‚Äúlisten‚Äù to people making real-life decisions and detect these psychological states 
                 from their descriptions.<p>
        </div>
        <div class="talk" id="talk6">
            <h3><strong>Quantifying Psychological Mechanisms in Real-Life
                 Decision-Making: A Novel Approach Leveraging Mobile Assessments 
                 and Large Language Models.</strong></h3>
            <p class="author"><em>Renato Frey, Olivia Fischer, and Aaron Lob, University of Zurich</em></p>
            <p>There are several psychological mechanisms that affect decisions in dynamic, real-life situations:</p>
            <ul>
                <li>affect, time pressure, goals and motivation, cognitive resources and control,
                    experience and knowledge</li>
            </ul> 
            <p><strong>üî¨ Study: </strong>Researchers used ecological momentary assessments, asking participants 
                to use LLMs whenever they were about to make a decision and speak out loud to the app. 
                Researchers then perform transcription and quantify semantic indicators in the language 
                related to key psychological factors.</p>
            <p><strong>‚úÖ Results: </strong> By analyzing the transcripts, the LLM could accurately 
                reverse-engineer the original context of the decision (e.g., inferring if the participant 
                pursuing a specific goal)</p>
            <p><strong>üí° Theoretical implications:</strong> We need to consider various dimensions that affect 
                ‚Äúrisky‚Äù decisions. Future research can manipulate one or more of the specific dimensions and
                 test causal mechanisms shaping decisions.</p>
            <p><strong>üöÄ AI applications:</strong> highlights how we can use LLMs in research, especially 
                for more ecologically valid methods such as experience sampling. LLMs can help 
                with creating semantic networks and relationships between clusters when dealing with large datasets</p>
            <a href="https://explorer.cbdr-lab.net/resultsselect" target="_blank"><strong>
                üîó See for more</strong></a>
        </div>
    </body>
    <footer>
    <p style="text-align: center; color: #95a5a6;">
        ‚Äî End of Notes ‚Äî
    </p>
</footer>
</html>
